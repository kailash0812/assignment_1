{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import csv\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# Initialize the Chrome driver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Open the URL in the browser\n",
    "driver.get('https://www.redbus.in/online-booking/apsrtc/?utm_source=rtchometile') # URL for scraping the government bus data. Paste the link as per other government bus services to scrape.\n",
    "driver.maximize_window()\n",
    "time.sleep(10)\n",
    "\n",
    "# Function to extract routes from the current page\n",
    "def extract_routes():\n",
    "    elements = driver.find_elements(By.XPATH, \"//a[@class='route']\")\n",
    "    return [{'text': element.text, 'link': element.get_attribute('href')} for element in elements]\n",
    "\n",
    "# Initialize routes list\n",
    "all_routes = []\n",
    "\n",
    "# Try to navigate through pages and capture routes\n",
    "page_xpaths = [\n",
    "    '//*[@id=\"root\"]/div/div[4]/div[12]/div[2]',  # Page 2\n",
    "    '//*[@id=\"root\"]/div/div[4]/div[12]/div[3]',  # Page 3\n",
    "    '//*[@id=\"root\"]/div/div[4]/div[12]/div[4]',  # Page 4\n",
    "    '//*[@id=\"root\"]/div/div[4]/div[12]/div[5]'   # Page 5\n",
    "]\n",
    "\n",
    "# Extract routes from the first page\n",
    "all_routes.extend(extract_routes())\n",
    "\n",
    "# Loop through each page's XPath and extract routes\n",
    "for page_xpath in page_xpaths:\n",
    "    try:\n",
    "        # Wait for the element to be clickable\n",
    "        element = WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, page_xpath)))\n",
    "\n",
    "        # Scroll the element into view\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView(true);\", element)\n",
    "        time.sleep(2)\n",
    "\n",
    "        # Click the element using JavaScript\n",
    "        driver.execute_script(\"arguments[0].click();\", element)\n",
    "        time.sleep(10)\n",
    "\n",
    "        # Extract routes from the current page\n",
    "        all_routes.extend(extract_routes())\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Could not navigate to the next page or extract routes: {e}\")\n",
    "        continue\n",
    "\n",
    "# Prepare the CSV file\n",
    "csv_file_path = 'apsrtc_bus_data.csv'\n",
    "with open(csv_file_path, 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Route Name', 'Route Link', 'Bus Name', 'Bus Type', 'Departing Time', 'Duration', 'Reaching Time', 'Star Rating', 'Price', 'Seats Available'])\n",
    "\n",
    "    for route in all_routes:\n",
    "        # Navigate to the route page\n",
    "        driver.get(route['link'])\n",
    "        time.sleep(10)\n",
    "\n",
    "        try:\n",
    "            # Attempt to click on the \"View Buses\" element if it exists\n",
    "            view_buses_xpath = '//*[@id=\"result-section\"]/div[1]/div/div[2]/div/div[4]/div[2]'\n",
    "            view_buses = WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, view_buses_xpath)))\n",
    "            view_buses.click()\n",
    "            time.sleep(5)\n",
    "        except TimeoutException:\n",
    "            print(f\"'View Buses' button not found for route {route['text']}. Extracting available data.\")\n",
    "\n",
    "        # Scroll down to the bottom of the page to load all content\n",
    "        SCROLL_PAUSE_TIME = 5\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        while True:\n",
    "            # Scroll down to the bottom\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(SCROLL_PAUSE_TIME)\n",
    "            \n",
    "            # Calculate new scroll height and compare with last scroll height\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "            last_height = new_height\n",
    "\n",
    "        # Extract data after reaching the bottom of the page\n",
    "        def get_elements(xpath):\n",
    "            return [elem.text for elem in driver.find_elements(By.XPATH, xpath)]\n",
    "\n",
    "        busname_xpath = \"//div[@class='travels lh-24 f-bold d-color']\"\n",
    "        bustype_xpath = \"//*[@class='bus-type f-12 m-top-16 l-color evBus']\"\n",
    "        departing_time_xpath = \"//*[@class='dp-time f-19 d-color f-bold']\"\n",
    "        duration_xpath = \"//*[@class='dur l-color lh-24']\"\n",
    "        reaching_time_xpath = \"//*[@class='bp-time f-19 d-color disp-Inline']\"\n",
    "        star_rating_xpath = \"//*[@class='column-six p-right-10 w-10 fl']\"\n",
    "        price_xpath = \"//*[contains(@class, 'fare d-block')]\"\n",
    "        seats_available_xpath = \"//*[@class='column-eight w-15 fl']\"\n",
    "\n",
    "        busnames = get_elements(busname_xpath)\n",
    "        bustypes = get_elements(bustype_xpath)\n",
    "        departing_times = get_elements(departing_time_xpath)\n",
    "        durations = get_elements(duration_xpath)\n",
    "        reaching_times = get_elements(reaching_time_xpath)\n",
    "        star_ratings = get_elements(star_rating_xpath)\n",
    "        prices = get_elements(price_xpath)\n",
    "        seats_available = get_elements(seats_available_xpath)\n",
    "\n",
    "        # Define the clean_price function\n",
    "        def clean_price(price_text):\n",
    "            return re.sub(r'[^0-9.]', '', price_text.strip())\n",
    "\n",
    "        # Clean and extend lists\n",
    "        prices = [clean_price(price) for price in prices]\n",
    "\n",
    "        length = max(len(busnames), len(bustypes), len(departing_times), len(durations), len(reaching_times), len(star_ratings), len(prices), len(seats_available))\n",
    "\n",
    "        def extend_list(lst, length):\n",
    "            return lst + ['N/A'] * (length - len(lst))\n",
    "\n",
    "        busnames = extend_list(busnames, length)\n",
    "        bustypes = extend_list(bustypes, length)\n",
    "        departing_times = extend_list(departing_times, length)\n",
    "        durations = extend_list(durations, length)\n",
    "        reaching_times = extend_list(reaching_times, length)\n",
    "        star_ratings = extend_list(star_ratings, length)\n",
    "        prices = extend_list(prices, length)\n",
    "        seats_available = extend_list(seats_available, length)\n",
    "\n",
    "        # Write data to CSV\n",
    "        for i in range(length):\n",
    "            writer.writerow([\n",
    "                route['text'],\n",
    "                route['link'],\n",
    "                busnames[i],\n",
    "                bustypes[i],\n",
    "                departing_times[i],\n",
    "                durations[i],\n",
    "                reaching_times[i],\n",
    "                star_ratings[i],\n",
    "                prices[i],\n",
    "                seats_available[i]\n",
    "            ])\n",
    "\n",
    "        # Go back to the main route list\n",
    "        driver.back()\n",
    "        time.sleep(10) \n",
    "\n",
    "# Close the browser\n",
    "driver.quit()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
